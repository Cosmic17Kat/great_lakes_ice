{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import datetime as dt\n",
    "import time\n",
    "import json\n",
    "from config import noaa_token as token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables\n",
    "base = 'https://www.ncdc.noaa.gov/cdo-web/api/v2/data'\n",
    "header = {'token':token,\n",
    "          'Content-Type':'application/json'}\n",
    "\n",
    "datasets = {'Daily':'GHCND',\n",
    "           'Monthly':'GSOM'}\n",
    "\n",
    "state_ids = {'PA':'FIPS:42',\n",
    "            'OH':'FIPS:39',\n",
    "            'MI':'FIPS:26',\n",
    "            'IL':'FIPS:17',\n",
    "            'WI':'FIPS:55',\n",
    "            'MN':'FIPS:27',\n",
    "            'IA':'FIPS:18',\n",
    "            'NY':'FIPS:36'}\n",
    "\n",
    "lake_st = {'Superior':['MI','MN','WI'],\n",
    "          'Michigan':['IA','IL','MI','WI'],\n",
    "          'Huron':['MI'],\n",
    "          'Erie':['IA','NY','OH','PA'],\n",
    "          'Ontario':['NY']}\n",
    "\n",
    "files_dir = os.path.join('..','data_files','api_calls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DT32</th>\n",
       "      <td>GSOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DX32</th>\n",
       "      <td>GSOM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRCP</th>\n",
       "      <td>GHCND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SNWD</th>\n",
       "      <td>GHCND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>THIC</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TMIN</th>\n",
       "      <td>GHCND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WDMV</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     dataset\n",
       "id          \n",
       "DT32    GSOM\n",
       "DX32    GSOM\n",
       "PRCP   GHCND\n",
       "SNWD   GHCND\n",
       "THIC     NaN\n",
       "TMIN   GHCND\n",
       "WDMV     NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables = pd.read_csv(os.path.join('..', 'data_files','prep_data','variables_to_use.csv'))\n",
    "variables['dataset'] = ['GSOM','GSOM','GHCND','GHCND',np.nan,'GHCND',np.nan]\n",
    "variable_ids = variables[['id','dataset']].set_index('id')\n",
    "variable_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Entries</th>\n",
       "      <th>Days in Range</th>\n",
       "      <th>% Coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1972-12-19</td>\n",
       "      <td>1973-04-07</td>\n",
       "      <td>18</td>\n",
       "      <td>109</td>\n",
       "      <td>16.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1973-12-30</td>\n",
       "      <td>1974-05-03</td>\n",
       "      <td>20</td>\n",
       "      <td>124</td>\n",
       "      <td>16.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1975-01-01</td>\n",
       "      <td>1975-04-25</td>\n",
       "      <td>19</td>\n",
       "      <td>114</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1975-12-22</td>\n",
       "      <td>1976-04-21</td>\n",
       "      <td>20</td>\n",
       "      <td>121</td>\n",
       "      <td>16.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1976-12-15</td>\n",
       "      <td>1977-05-05</td>\n",
       "      <td>23</td>\n",
       "      <td>141</td>\n",
       "      <td>16.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       Start         End  Entries  Days in Range  % Coverage\n",
       "0           0  1972-12-19  1973-04-07       18            109       16.51\n",
       "1           1  1973-12-30  1974-05-03       20            124       16.13\n",
       "2           2  1975-01-01  1975-04-25       19            114       16.67\n",
       "3           3  1975-12-22  1976-04-21       20            121       16.53\n",
       "4           4  1976-12-15  1977-05-05       23            141       16.31"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phases_df = pd.read_csv(os.path.join('..', 'data_files','prep_data','data_collection_phases.csv'))\n",
    "phases_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect all of the calls needed to gather the data for a given lake and year range (called phases from here on), we've created a series of functions to create and group the calls in a series of nested lists.\n",
    "\n",
    "The top list represents individual variables, as seen in the variables_id dataframe above.\n",
    "The second layer list represents individual states that border the selected lake, identified in a dictionary in the second cell.\n",
    "The third layer is comprised of dictionaries, one per phase given the state and variable in the above layers.\n",
    "\n",
    "Each of these functions are named their layer (ex: variable, state, phase) followed by 'load' as the dictionaries will be used as payloads for the api calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns list of calls, 1 call per phase for a specified state and data_id\n",
    "def phase_loads(st,phase_start,phase_end,data_id):\n",
    "    load = {}\n",
    "    st_load = []\n",
    "    for i in range(phase_start,phase_end):\n",
    "        load = {'limit':1000,\n",
    "        'offset':0,\n",
    "        'includeAttributes':'true',\n",
    "        'datatypeid': data_id,\n",
    "        'datasetid': variable_ids.loc[data_id],\n",
    "        'startdate': phases_df.iloc[i]['Start'],\n",
    "        'enddate': phases_df.iloc[i]['End'],\n",
    "        'locationid': state_ids[st]}\n",
    "        st_load.append(load)\n",
    "    return st_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns nested list of calls, st/phase\n",
    "def state_loads(lake,phase_start,phase_end,data_id):\n",
    "    \"\"\"lake is a string name of a Great Lake wihtout the preceding 'Lake'.\n",
    "    phase_start should be an index from the data_collection_phases.csv (int).\n",
    "    phase_end is exclusive index from the same csv (int).\n",
    "    data_id should be an ID from the variables_to_use.csv.\n",
    "    Returns list of list of of dictionaries to be used for a series of api calls.\"\"\"\n",
    "    lk_load = []\n",
    "    lake_borders = lake_st[lake]\n",
    "    for e in lake_borders:\n",
    "        lk_load.append(phase_loads(e,phase_start,phase_end,data_id))\n",
    "    return lk_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_loads(lake,phase_start,phase_end,dataset=None):\n",
    "    var_load = []\n",
    "    for index, row in variable_ids.iterrows():\n",
    "        if isinstance(row['dataset'],str):\n",
    "            if row['dataset'] == dataset or dataset is None:\n",
    "                var_load.append(state_loads(lake,phase_start,phase_end,index))\n",
    "    print(f'{lake} loads collected.')\n",
    "    return var_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To limit the number of api calls needed, we are saving batches at each level in specific directories.\n",
    "To this end, we need to generate filenames in a standard way and then verify whether the file already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_file(filename):\n",
    "    if os.path.isfile(filename):\n",
    "        print(f\"There's already a file for this: {filename}\")\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The api limits the results within the json to a max of 1000 entries.\n",
    "This means, for a given phase/state/variable, we need to make a series of calls, updating the offset to pull new data.\n",
    "\n",
    "To know how far to go, we have a get_c function, to identify the total number of entries.\n",
    "Then there is the function that makes the individual call, max size = 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_c(load):\n",
    "    load['limit'] = 5\n",
    "    load['offset']=0\n",
    "    c = requests.get(base,headers=header,params=load).json()['metadata']['resultset']['count']\n",
    "    load['limit'] = 1000\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export(file_path,file,data):\n",
    "    if is_not_file(os.path.join(file_path,file+'.csv')):\n",
    "        data.to_csv(os.path.join(file_path,file+'.csv'))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exports 1 call of max 1000 entries to csv in data_files/api_calls/individual_calls\n",
    "def call(load,filename):\n",
    "    r = requests.get(base,headers=header,params=load).json()\n",
    "    results = r['results']\n",
    "    data = pd.DataFrame(results)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To limit the number of api calls, here is where we conditionally call.\n",
    "This function also provides progress percentage, to keep the user updated on where they are in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(count,load,file):\n",
    "    file = file + '_' + str(load['offset'])\n",
    "    df = pd.DataFrame()\n",
    "    if is_not_file(os.path.join(files_dir,'individual_calls',file)) & count>0:\n",
    "        df = call(load,file)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we layered the calls in nested lists, we need to unpack them slowly in a controlled manner.\n",
    "We share which phase/state/variable we are currently processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exports 1 call, all entries in a given time range to csv in data_files/api_calls/call_batches\n",
    "#Takes dictionary\n",
    "def phase_call(load,filebase,status=True):\n",
    "    phase = phases_df.index[phases_df['Start']==load['startdate']].tolist()[0]\n",
    "    file = filebase + '_' + str(phase)\n",
    "    \n",
    "    try:\n",
    "        count = get_c(load)\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    load['offset'] = 0\n",
    "    total_data = pd.DataFrame()\n",
    "    if status:\n",
    "        print(f\"Phase {phase} start: \")\n",
    "    while load['offset'] < count:\n",
    "        if status:\n",
    "            print(f\"Initializing call {(load['offset']//1000)+1} of {(count//1000)+1}\")\n",
    "        try:\n",
    "            df = check(count,load,file)\n",
    "        except:\n",
    "            export(files_dir,f'FAILED_{dt.datetime.now()}_'+file,total_data)\n",
    "            print(f\"Failed at {load['offset']}, {phase}, {load['locationid']}, {load['datatypeid']}\")\n",
    "            return total_data\n",
    "        \n",
    "        total_data = total_data.append(df)\n",
    "        load['offset'] = load['offset']+load['limit']\n",
    "    \n",
    "    if status:\n",
    "        print(f\"Phase {phase} complete.\")\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes list of dictionaries\n",
    "def state_calls(st_ls,filebase,status=True):\n",
    "    file = filebase + '_' + st_ls[0]['locationid']\n",
    "    total_data = pd.DataFrame()\n",
    "    if status:\n",
    "        print(f\"State {st_ls[0]['locationid']} start.\")\n",
    "    for i in range(len(st_ls)):\n",
    "        if status:\n",
    "            print(f\"Initializing call {i+1} of {len(st_ls)}\")\n",
    "        try:\n",
    "            total_data = total_data.append(phase_call(st_ls[i],file,status))\n",
    "        except:\n",
    "            export(files_dir,f'FAILED_{dt.datetime.now()}_'+file,total_data)\n",
    "            print(f\"Failed at {load['locationid']}, {load['datatypeid']}\")\n",
    "            return total_data\n",
    "        \n",
    "    if status:\n",
    "        print(f\"State {st_ls[0]['locationid']} complete.\")\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exports a series call, all entries in a given time range for all states bordering a specific lake \n",
    "# to csv in data_files/api_calls\n",
    "#Takes list of list of dictionaries\n",
    "def variable_calls(var_ls,filebase,status=True):\n",
    "    file = filebase + '_' + var_ls[0][0]['datatypeid']\n",
    "    total_data = pd.DataFrame()\n",
    "    if status:\n",
    "        print(f\"Variable {var_ls[0][0]['datatypeid']} start.\")\n",
    "    for i in range(len(var_ls)):\n",
    "        if status:\n",
    "            print(f\"Initializing call {i+1} of {len(var_ls)}\")\n",
    "        try:\n",
    "            total_data = total_data.append(state_calls(var_ls[i],file,status))\n",
    "        except:\n",
    "            export(files_dir,f'FAILED_{dt.datetime.now()}_'+file,total_data)\n",
    "            print(f\"Failed at {load['datatypeid']}\")\n",
    "            return total_data\n",
    "        \n",
    "    if status:\n",
    "        print(f\"Variable {var_ls[0][0]['datatypeid']} complete.\")\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes list of list of list of dictionaries\n",
    "def lake_calls(lk_ls,lake_name,status=True):\n",
    "    phase_start = lk_ls[0][0][0]['startdate']\n",
    "    phase_end = lk_ls[-1][-1][-1]['enddate']\n",
    "    file = lake_name + '_' + phase_start + '_' + phase_end\n",
    "    total_data = pd.DataFrame()\n",
    "    if status:\n",
    "        print(f\"Lake {lake_name} start.\")\n",
    "    for lake in lk_ls:\n",
    "        try:\n",
    "            total_data = total_data.append(variable_calls(lake,file,status))\n",
    "        except:\n",
    "            export(files_dir,f'FAILED_{dt.datetime.now()}_'+file,total_data)\n",
    "            print(f\"Failed at {lake}\")\n",
    "            return total_data\n",
    "    if status:\n",
    "        print(f\"Lake {lake_name} complete.\")\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_calls(lakes,start_phase,end_phase,dataset,status=True):\n",
    "    total_data = pd.DataFrame()\n",
    "    for lake in lakes:\n",
    "        file = f'{lake}_{start_phase}-{end_phase}_{dataset}'\n",
    "        if is_not_file(os.path.join(files_dir,file+'.csv')):\n",
    "            total_data = total_data.append(\n",
    "                lake_calls(\n",
    "                    var_loads(\n",
    "                        lake,start_phase,end_phase,dataset=dataset),\n",
    "                    lake,status))\n",
    "            export(files_dir,file,total_data)\n",
    "        if status:\n",
    "            print(f\"{lake} finished at {dt.datetime.now()}\")\n",
    "    print(f'{len(total_data)} records gathered and exported.')\n",
    "    return total_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all that, we're finally down to the action.\n",
    "Here is where we'll generate the nested lists for our chosen lake and in the following cell, we'll use all the code you've skimmed past to pull, save, and export the data.\n",
    "Just be careful, there's no error handling at all here, if you're not careful, you may stop the whole process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Superior loads collected.\n",
      "Failed at 0, 24, FIPS:26, DT32\n",
      "Failed at 0, 25, FIPS:55, DT32\n",
      "Failed at 0, 26, FIPS:55, DT32\n",
      "Failed at 0, 22, FIPS:26, DX32\n",
      "Failed at 0, 24, FIPS:26, DX32\n",
      "Failed at 0, 25, FIPS:26, DX32\n",
      "Failed at 0, 26, FIPS:26, DX32\n",
      "Failed at 0, 23, FIPS:27, DX32\n",
      "Failed at 0, 25, FIPS:27, DX32\n",
      "Failed at 0, 25, FIPS:55, DX32\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-2fa5f33ac54d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmake_calls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlakes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_phase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_phase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mfinish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-3016408b6d82>\u001b[0m in \u001b[0;36mmake_calls\u001b[0;34m(lakes, start_phase, end_phase, dataset, status)\u001b[0m\n\u001b[1;32m      9\u001b[0m                         lake,start_phase,end_phase,dataset=dataset),\n\u001b[1;32m     10\u001b[0m                     lake,status))\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{lake} finished at {dt.datetime.now()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "start_phase = 22\n",
    "end_phase = 27\n",
    "lakes = ['Superior','Michigan','Huron','Erie','Ontario']\n",
    "dataset='GSOM'\n",
    "\n",
    "start = dt.datetime.now()\n",
    "make_calls(lakes,start_phase,end_phase,dataset,status=False)\n",
    "finish = dt.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(finish-start).seconds/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
